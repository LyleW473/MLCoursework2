{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2004)\n",
    "random.seed(2004)\n",
    "np.random.seed(2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 # Assumed\n",
    "LR = 0.025 # Same as paper\n",
    "MOMENTUM = 0.9 # Same as paper\n",
    "USE_NESTEROV = True # Same as paper\n",
    "EPOCHS = 100\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRINT_INTERVAL = 1000\n",
    "\n",
    "MODEL_SAVE_DIR = \"models\"\n",
    "CLASSES = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load active learning embeddings and build dataset\n",
    "embeddings_dir = \"embeddings/100_iterations\"\n",
    "if not os.path.exists(f\"embeddings/100_iterations\"):\n",
    "    raise ValueError(\"Please run build_dataset.py first to generate embeddings for active learning\")\n",
    "\n",
    "active_learning_embeddings = {}\n",
    "for i in range(100):\n",
    "    with open(f\"{embeddings_dir}/embedding_{i}.pkl\", \"rb\") as f:\n",
    "        embeddings = pickle.load(f)\n",
    "        active_learning_embeddings[i] = embeddings\n",
    "\n",
    "print(\"Embeddings shape:\", len(active_learning_embeddings))\n",
    "\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for key, value_dict in active_learning_embeddings.items():\n",
    "    image = value_dict[\"image\"]\n",
    "    print(image.flatten().min(), image.flatten().max())\n",
    "    image = Image.fromarray(image.astype(\"uint8\")) # Convert to PIL image\n",
    "    label = np.array(value_dict[\"label\"]) # Convert scalar to 1D array\n",
    "    print(np.array(image).shape, label.shape)\n",
    "    all_images.append(image)\n",
    "    all_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train and validation sets:\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(all_images, all_labels, test_size=0.2, random_state=2004)\n",
    "\n",
    "print(f\"Number of training images: {len(train_images)}\")\n",
    "print(f\"Number of validation images: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances_per_class(labels):\n",
    "    class_counts = defaultdict(int)\n",
    "    for label in labels:\n",
    "        class_counts[CLASSES[label.item()]] += 1\n",
    "    return dict(class_counts)\n",
    "\n",
    "def print_class_counts(class_counts, labels):\n",
    "    for c_class, count in class_counts.items():\n",
    "        print(f\"Class: {c_class} | Count: {count} | Percentage: {count / len(labels) * 100:.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_counts = count_instances_per_class(train_labels)\n",
    "val_class_counts = count_instances_per_class(val_labels)\n",
    "\n",
    "print(\"Training class counts:\")\n",
    "print_class_counts(train_class_counts, train_labels)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Validation class counts:\")\n",
    "print_class_counts(val_class_counts, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=0), # Padding not mentioned in paper\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = torch.tensor(label)\n",
    "        return image, label\n",
    "    \n",
    "# train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_set = CustomDataset(images=train_images, labels=train_labels, transform=transform)\n",
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "val_set = CustomDataset(images=val_images, labels=val_labels, transform=transform)\n",
    "val_dl = torch.utils.data.DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_dl = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl))\n",
    "print(len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dl.__iter__().__next__()\n",
    "images, labels = data\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # Unnormalize\n",
    "    np_img = img.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "data_iter = iter(train_dl)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\" \".join(f\"{CLASSES[labels[j]]:5s}\" for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet18_model():\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(512, 10) # Adapt final layer to CIFAR-10 classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_resnet18_model()\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, nesterov=USE_NESTEROV)\n",
    "scheduler = CosineAnnealingLR(optimiser, T_max=EPOCHS) # T_max is the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_batches = len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_positive, false_positive, false_negative, total):\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1_score = {}\n",
    "    accuracy = {}\n",
    "        \n",
    "    for class_name in CLASSES:\n",
    "        tp = true_positive[class_name]\n",
    "        fp = false_positive[class_name]\n",
    "        fn = false_negative[class_name]\n",
    "        total_count = total[class_name]\n",
    "\n",
    "        # Accuracy (same as existing code)\n",
    "        accuracy[class_name] = 100 * float(tp) / total_count if total_count != 0 else 0\n",
    "\n",
    "        # Precision = TP / (TP + FP)\n",
    "        precision[class_name] = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "\n",
    "        # Recall = TP / (TP + FN)\n",
    "        recall[class_name] = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "        # F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        if (precision[class_name] + recall[class_name]) > 0:\n",
    "            f1_score[class_name] = 2 * (precision[class_name] * recall[class_name]) / (precision[class_name] + recall[class_name])\n",
    "        else:\n",
    "            f1_score[class_name] = 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_forward_pass(model, criterion, optimiser, data_loader, epoch, num_batches, mode=\"train\"):\n",
    "\n",
    "    total = {class_name: 0 for class_name in CLASSES}\n",
    "    true_positive = defaultdict(int)\n",
    "    false_positive = defaultdict(int)\n",
    "    false_negative = defaultdict(int)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, labels = data\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        output = model(inputs)\n",
    "\n",
    "        # Count true positives, false positives, and false negatives\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            total[CLASSES[label]] += 1 # Track total for accuracy\n",
    "\n",
    "            if label == prediction:\n",
    "                true_positive[CLASSES[label]] += 1\n",
    "            else:\n",
    "                false_positive[CLASSES[prediction]] += 1 # Predicted class is wrong\n",
    "                false_negative[CLASSES[label]] += 1 # Actual class is wrong\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_examples += len(labels)\n",
    "\n",
    "        if (i % PRINT_INTERVAL == (PRINT_INTERVAL - 1)) or (i == num_batches - 1):\n",
    "            current_lr = optimiser.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch: {epoch + 1} |  Batch: {i + 1}/{num_batches} | Loss: {running_loss / total_examples} | LR: {current_lr}\")\n",
    "\n",
    "    # Calculate metrics after each epoch\n",
    "    precision, recall, f1_score, accuracy = calculate_metrics(\n",
    "                                                            true_positive=true_positive, \n",
    "                                                            false_positive=false_positive, \n",
    "                                                            false_negative=false_negative, \n",
    "                                                            total=total\n",
    "                                                            )\n",
    "    average_epoch_loss = running_loss / total_examples\n",
    "    return average_epoch_loss, precision, recall, f1_score, accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, train_precision, train_recall, train_f1_score, train_accuracy = epoch_forward_pass(\n",
    "                                                                                    model=model,\n",
    "                                                                                    criterion=criterion,\n",
    "                                                                                    optimiser=optimiser,\n",
    "                                                                                    data_loader=train_dl,\n",
    "                                                                                    epoch=epoch,\n",
    "                                                                                    num_batches=len(train_dl),\n",
    "                                                                                    mode=\"train\"\n",
    "                                                                                    )\n",
    "    # Print metrics for each class:\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    print(f\"Training metrics\")\n",
    "    print(f\"Average train loss: {train_loss:.4f}\")\n",
    "    for class_name in CLASSES:\n",
    "        print(f\"Class: {class_name} | Precision: {train_precision[class_name]:.2f} | Recall: {train_recall[class_name]:.2f} | F1 Score: {train_f1_score[class_name]:.2f} | Accuracy: {train_accuracy[class_name]:.2f}\")\n",
    "    \n",
    "    # Validate model after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_precision, val_recall, val_f1_score, val_accuracy = epoch_forward_pass(\n",
    "                                                                                            model=model,\n",
    "                                                                                            criterion=criterion,\n",
    "                                                                                            optimiser=optimiser,\n",
    "                                                                                            data_loader=val_dl,\n",
    "                                                                                            epoch=epoch,\n",
    "                                                                                            num_batches=len(val_dl),\n",
    "                                                                                            mode=\"val\"\n",
    "                                                                                            )\n",
    "    # Print metrics for each class:\n",
    "    print(f\"Validation metrics\")\n",
    "    print(f\"Average val loss: {val_loss:.4f}\")\n",
    "    for class_name in CLASSES:\n",
    "        print(f\"Class: {class_name} | Precision: {val_precision[class_name]:.2f} | Recall: {val_recall[class_name]:.2f} | F1 Score: {val_f1_score[class_name]:.2f} | Accuracy: {val_accuracy[class_name]:.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Update scheduler after each epoch\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "model_path = f\"{MODEL_SAVE_DIR}/fully_supervised_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(test_dl)\n",
    "images, labels = next(data_iter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(f\"Ground truth: {', '.join(f'{CLASSES[labels[j]]:5s}' for j in range(BATCH_SIZE))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = create_resnet18_model()\n",
    "saved_model.load_state_dict(torch.load(model_path))\n",
    "saved_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = saved_model(images.to(DEVICE))\n",
    "_, predicted = torch.max(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Predictions: {', '.join(f'{CLASSES[predicted[j]]:5s}' for j in range(BATCH_SIZE))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = {class_name: 0 for class_name in CLASSES}\n",
    "true_positive = defaultdict(int)\n",
    "false_positive = defaultdict(int)\n",
    "false_negative = defaultdict(int)\n",
    "\n",
    "saved_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        output = saved_model(images)\n",
    "\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            total[CLASSES[label]] += 1 # Track total for accuracy\n",
    "\n",
    "            if label == prediction:\n",
    "                true_positive[CLASSES[label]] += 1\n",
    "            else:\n",
    "                false_positive[CLASSES[prediction]] += 1 # Predicted class is wrong\n",
    "                false_negative[CLASSES[label]] += 1 # Actual class is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1_score, accuracy = calculate_metrics(\n",
    "                                                        true_positive=true_positive, \n",
    "                                                        false_positive=false_positive, \n",
    "                                                        false_negative=false_negative, \n",
    "                                                        total=total\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in total.keys():\n",
    "    print(f\"Class name: {class_name}\")\n",
    "    print(f\"Accuracy: {accuracy[class_name]:.4f}\")\n",
    "    print(f\"Precision: {precision[class_name]:.4f}\")\n",
    "    print(f\"Recall: {recall[class_name]:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score[class_name]:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
